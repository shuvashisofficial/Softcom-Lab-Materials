{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the Spark file to the current folder\n",
        "!tar xf spark-3.0.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwFN3NqF-Zx0",
        "outputId": "e2c83cd0-2f09-46a1-bd45-ccc40e25362d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('/content/Online Retail.csv',header=True,escape=\"\\\"\")\n",
        "df.show(5,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX6N9vhOF5dZ",
        "outputId": "ada811f6-88b1-4aed-dd0b-f0d466a1bcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
            "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
            "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
            "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n",
            "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
            "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
            "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [['a', 1, 2], ['b', 2, 3],['c', 3, 4]]\n",
        "col_name = ['A', 'B', 'C']\n",
        "spark.createDataFrame(my_list, col_name).show()\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(my_list,columns= col_name)\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSfWKq5EOXT4",
        "outputId": "c0d6d653-de28-4e56-9981-564b0fca9b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "|  A|  B|  C|\n",
            "+---+---+---+\n",
            "|  a|  1|  2|\n",
            "|  b|  2|  3|\n",
            "|  c|  3|  4|\n",
            "+---+---+---+\n",
            "\n",
            "   A  B  C\n",
            "0  a  1  2\n",
            "1  b  2  3\n",
            "2  c  3  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(my_list,col_name)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrvC9kyzPAzY",
        "outputId": "e73fb912-21f2-40bb-bb8b-9c8ce5d0d6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0  1  2\n",
            "A  a  1  2\n",
            "B  b  2  3\n",
            "C  c  3  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# df = spark.createDataFrame(pandas_df.toPandas())\n",
        "# Creating pandas dataframe first\n",
        "l = [[\"2015-06-23\", 5]\n",
        " ,[\"2016-07-20\", 7]] #List with data elements\n",
        "df = spark.createDataFrame(pd.DataFrame(l),['data_date','months_to_add'])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tae1YLWlOZeA",
        "outputId": "dfc14d1a-54d8-4a58-b130-afd4277c304f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:327: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spark libraries\n",
        "from pyspark.sql import Row, DataFrame\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
        "# create RDD to load into spark dataframe\n",
        "l = [[\"2015-06-23\", 5] , [\"2016-07-20\", 7]] #List with data elements\n",
        "rdd1 = spark.sparkContext.parallelize(l)\n",
        "row_rdd = rdd1.map(lambda x: Row(x[0], x[1]))\n",
        "df = spark.createDataFrame(row_rdd, ['data_date', 'months_to_add'])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzNfLWQQPXCA",
        "outputId": "ea75828a-56e5-4d74-c29c-654350f88abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spark libraries\n",
        "from pyspark.sql import Row, DataFrame\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
        "# create RDD to load into spark dataframe\n",
        "l = [[\"2015-06-23\", 5],[\"2016-07-20\", 7]] #List with data elements\n",
        "rdd1 = spark.sparkContext.parallelize(l)\n",
        "schema = StructType([ StructField(\"data_date\", StringType(), True),\n",
        "StructField(\"months_to_add\", IntegerType(), True)]) # Col, Type, Nullable\n",
        "df = spark.createDataFrame(rdd1, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfNdF8oEPiwe",
        "outputId": "ce0c42df-42fc-4917-e110-38dae0e88fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
        "rdd=spark.sparkContext.parallelize(dataList)\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K5fJp6zPqAH",
        "outputId": "3f6afee3-2f04-4d04-96c9-492b5ca7867c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = spark.sparkContext.textFile(\"C:\\spark\\text1.txt\")\n",
        "df1=spark.read.text(\"/text1.txt\")\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "l5ibC3wsQIaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "# function to create new SparkSession\n",
        "def create_session():\n",
        " spk = SparkSession.builder \\\n",
        " .appName(\"Corona_cases_statewise.com\") \\\n",
        " .getOrCreate()\n",
        " return spk\n",
        "# function to create RDD\n",
        "def create_RDD(sc_obj, data):\n",
        " df = sc.parallelize(data)\n",
        " return df\n",
        " # function to convert RDD to dataframe\n",
        "def RDD_to_df(df,schema):\n",
        "# converting RDD to dataframe using toDF() in which we are passing schema of df\n",
        " df3 = rd_df.toDF(schema)\n",
        " return df3\n",
        "if __name__ == \"__main__\":\n",
        " input_data = [(\"Dhaka\", 122000, 89600, 12238), (\"Khulna\", 454000, 380000, 67985),\n",
        " (\"Rangpur\", 115000, 102000, 13933),(\"Bogra\", 147000, 111000, 15306),\n",
        " (\"Rajshahi\", 153000, 124000, 5259)]\n",
        " # calling function to create SparkSession\n",
        " spark = create_session()\n",
        " # creating spark context object\n",
        " sc = spark.sparkContext\n",
        " # calling function to create RDD\n",
        " rd_df = create_RDD(sc, input_data)\n",
        " # printing the type\n",
        " print(type(rd_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3IgfQVEQI1u",
        "outputId": "a414a245-85ef-4219-ff07-0d60b76d6ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert RDD to dataframe\n",
        "def RDD_to_df(spark,df,schema):\n",
        "# converting RDD to df using createDataframe()\n",
        "# in which we are passing RDD and schema of df\n",
        " df1 = spark.createDataFrame(df,schema)\n",
        " return df1\n",
        "\n",
        "schema_lst = [\"State\",\"Cases\",\"Recovered\",\"Deaths\"]\n",
        "# calling function to convert RDD to dataframe\n",
        "converted_df = RDD_to_df(spark,rd_df,schema_lst)\n",
        "# visualizing the schema and dataframe\n",
        "converted_df.printSchema()\n",
        "converted_df.show()\n",
        "\n",
        "\n",
        "# function to convert RDD to dataframe\n",
        "def RDD_to_df(df,schema):\n",
        "# converting RDD to dataframe using toDF()\n",
        "# in which we are passing schema of df\n",
        " df = rd_df.toDF(schema)\n",
        " return df\n",
        "\n",
        "schema_lst = [\"State\",\"Cases\",\"Recovered\",\"Deaths\"]\n",
        "# calling function to convert RDD to dataframe\n",
        "converted_df = RDD_to_df(rd_df,schema_lst)\n",
        "# visualizing the schema and dataframe\n",
        "converted_df.printSchema()\n",
        "converted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDg5_HicQcY_",
        "outputId": "0d01dad5-0bcf-4301-ad5c-be261674bcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- State: string (nullable = true)\n",
            " |-- Cases: long (nullable = true)\n",
            " |-- Recovered: long (nullable = true)\n",
            " |-- Deaths: long (nullable = true)\n",
            "\n",
            "+--------+------+---------+------+\n",
            "|   State| Cases|Recovered|Deaths|\n",
            "+--------+------+---------+------+\n",
            "|   Dhaka|122000|    89600| 12238|\n",
            "|  Khulna|454000|   380000| 67985|\n",
            "| Rangpur|115000|   102000| 13933|\n",
            "|   Bogra|147000|   111000| 15306|\n",
            "|Rajshahi|153000|   124000|  5259|\n",
            "+--------+------+---------+------+\n",
            "\n",
            "root\n",
            " |-- State: string (nullable = true)\n",
            " |-- Cases: long (nullable = true)\n",
            " |-- Recovered: long (nullable = true)\n",
            " |-- Deaths: long (nullable = true)\n",
            "\n",
            "+--------+------+---------+------+\n",
            "|   State| Cases|Recovered|Deaths|\n",
            "+--------+------+---------+------+\n",
            "|   Dhaka|122000|    89600| 12238|\n",
            "|  Khulna|454000|   380000| 67985|\n",
            "| Rangpur|115000|   102000| 13933|\n",
            "|   Bogra|147000|   111000| 15306|\n",
            "|Rajshahi|153000|   124000|  5259|\n",
            "+--------+------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = [1, 2, 3, 4]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "rdd_transformed = rdd.map(lambda x: x * 2)\n",
        "rdd_transformed.collect()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"map_example\").getOrCreate()\n",
        "# Create a DataFrame with sample data\n",
        "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "# Define a function to be applied to each row\n",
        "def add_one(age):\n",
        " return age + 1\n",
        "# Use the map() transformation to apply\n",
        "# the function to the \"age\" column\n",
        "df = df.rdd.map(lambda x: (x[0], add_one(x[1]))).toDF([\"name\", \"age\"])\n",
        "# Show the resulting DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUjSKx7dSSd-",
        "outputId": "c7c367e0-d8f8-46b4-8565-44ddf06354c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice|  2|\n",
            "|    Bob|  3|\n",
            "|Charlie|  4|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"Project Gutenberg’s\",\n",
        " \"Alice’s Adventures in Wonderland\",\n",
        " \"Project Gutenberg’s\",\n",
        " \"Adventures in Wonderland\",\n",
        " \"Project Gutenberg’s\"]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "for element in rdd.collect():\n",
        " print(element)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc5NrL9sSriu",
        "outputId": "785a331f-d259-4c9f-d70a-6cc8dd9877c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Gutenberg’s\n",
            "Alice’s Adventures in Wonderland\n",
            "Project Gutenberg’s\n",
            "Adventures in Wonderland\n",
            "Project Gutenberg’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "for element in rdd2.collect():\n",
        " print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouW9El9fS7b0",
        "outputId": "5538caa4-4856-4d20-84ab-74eb3421ba6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project\n",
            "Gutenberg’s\n",
            "Alice’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
        "arrayData = [\n",
        " ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
        " ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
        " ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
        " ('Washington',None,None),\n",
        " ('Jefferson',['1','2'],{})]\n",
        "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
        "from pyspark.sql.functions import explode\n",
        "df2 = df.select(df.name,explode(df.knownLanguages))\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu_egpp-Sz9r",
        "outputId": "911cbaf6-adf7-4a93-c1e4-4e7633b5ed34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- col: string (nullable = true)\n",
            "\n",
            "+---------+------+\n",
            "|     name|   col|\n",
            "+---------+------+\n",
            "|    James|  Java|\n",
            "|    James| Scala|\n",
            "|  Michael| Spark|\n",
            "|  Michael|  Java|\n",
            "|  Michael|  null|\n",
            "|   Robert|CSharp|\n",
            "|   Robert|      |\n",
            "|Jefferson|     1|\n",
            "|Jefferson|     2|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "# function to create SparkSession\n",
        "def create_session():\n",
        " spk = SparkSession.builder .master(\"local\").appName(\"Student_report.com\").getOrCreate()\n",
        " return spk\n",
        "def create_df(spark, data, schema):\n",
        " df1 = spark.createDataFrame(data, schema)\n",
        " return df1\n",
        "if __name__ == \"__main__\":\n",
        " # calling function to create SparkSession\n",
        " spark = create_session()\n",
        " input_data = [(1, \"Shivansh\", \"Male\", 20, 80),\n",
        " (2, \"Arpy\", \"Female\", 18, 66), (3, \"Raj\", \"Male\", 21, 90),\n",
        " (4, \"Shima\", \"Female\", 19, 91), (5, \"Apurba\", \"Male\", 20, 50),\n",
        " (6, \"Swapnil\", \"Male\", 23, 65), (7, \"Rajesh\", \"Male\", 19, 70)]\n",
        " schema = [\"Id\", \"Name\", \"Gender\", \"Age\", \"Percentage\"]\n",
        " # calling function to create dataframe\n",
        " df = create_df(spark, input_data, schema)\n",
        " df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K35S0mgcTJAr",
        "outputId": "0c704e21-6ec9-4480-fefb-54aba33f5ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+------+---+----------+\n",
            "| Id|    Name|Gender|Age|Percentage|\n",
            "+---+--------+------+---+----------+\n",
            "|  1|Shivansh|  Male| 20|        80|\n",
            "|  2|    Arpy|Female| 18|        66|\n",
            "|  3|     Raj|  Male| 21|        90|\n",
            "|  4|   Shima|Female| 19|        91|\n",
            "|  5|  Apurba|  Male| 20|        50|\n",
            "|  6| Swapnil|  Male| 23|        65|\n",
            "|  7|  Rajesh|  Male| 19|        70|\n",
            "+---+--------+------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# subset or filter the dataframe by\n",
        "# passing Multiple condition\n",
        "df = df.filter(\"Gender == 'Male' and Percentage>70\")\n",
        "df.show()\n",
        "# subset or filter the data with\n",
        "# multiple condition\n",
        "df = df.filter(\"Age>20 or Percentage>80\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anAAhdVjTLls",
        "outputId": "97ad3af2-b528-4544-f6f7-125b89332cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+------+---+----------+\n",
            "| Id|    Name|Gender|Age|Percentage|\n",
            "+---+--------+------+---+----------+\n",
            "|  1|Shivansh|  Male| 20|        80|\n",
            "|  3|     Raj|  Male| 21|        90|\n",
            "+---+--------+------+---+----------+\n",
            "\n",
            "+---+----+------+---+----------+\n",
            "| Id|Name|Gender|Age|Percentage|\n",
            "+---+----+------+---+----------+\n",
            "|  3| Raj|  Male| 21|        90|\n",
            "+---+----+------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# subset or filter the data with\n",
        "# multiple condition\n",
        "df = df.filter((df.Gender=='Male') | (df.Percentage>90))\n",
        "df.show()\n",
        "# subset or filter the dataframe by\n",
        "# passing Multiple condition\n",
        "df = df.filter((df.Gender=='Female') & (df.Age>=18))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvq558JWTSaL",
        "outputId": "2bc2fd72-aae9-4909-f9ef-234f9e363afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+---+----------+\n",
            "| Id|Name|Gender|Age|Percentage|\n",
            "+---+----+------+---+----------+\n",
            "|  3| Raj|  Male| 21|        90|\n",
            "+---+----+------+---+----------+\n",
            "\n",
            "+---+----+------+---+----------+\n",
            "| Id|Name|Gender|Age|Percentage|\n",
            "+---+----+------+---+----------+\n",
            "+---+----+------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing module\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "# creating sparksession and giving an app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "data = [[\"1\", \"Sourav\", \"IT\", 45000], [\"2\", \"Aditi\", \"IT\", 30000], [\"3\", \"Bobby\", \"business\", 45000],\n",
        " [\"4\", \"Rohit\", \"IT\", 45000], [\"5\", \"Shamim\", \"business\", 120000],\n",
        " [\"6\", \"Moni\", \"sales\", 23000], [\"7\", \"Borno\", \"sales\", 34000],\n",
        " [\"8\", \"Shreya\", \"business\", 456798], [\"9\", \"Kishor\", \"IT\", 230000],\n",
        " [\"10\", \"Ron\", \"business\", 100000] ]\n",
        "# specify column names\n",
        "columns = ['ID', 'NAME', 'sector', 'salary']\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "# display dataframe\n",
        "dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ps4O-gQTXlD",
        "outputId": "3d3bd9fa-040f-41ac-e998-93c3b413b8b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------+------+\n",
            "| ID|  NAME|  sector|salary|\n",
            "+---+------+--------+------+\n",
            "|  1|Sourav|      IT| 45000|\n",
            "|  2| Aditi|      IT| 30000|\n",
            "|  3| Bobby|business| 45000|\n",
            "|  4| Rohit|      IT| 45000|\n",
            "|  5|Shamim|business|120000|\n",
            "|  6|  Moni|   sales| 23000|\n",
            "|  7| Borno|   sales| 34000|\n",
            "|  8|Shreya|business|456798|\n",
            "|  9|Kishor|      IT|230000|\n",
            "| 10|   Ron|business|100000|\n",
            "+---+------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "# list of employee data with 10 row values\n",
        "data = [[\"1\", \"Sourav\", \"IT\", 45000], [\"2\", \"Aditi\", \"IT\", 30000], [\"3\", \"Bobby\", \"business\", 45000],\n",
        " [\"4\", \"Rohit\", \"IT\", 45000], [\"5\", \"Shamim\", \"business\", 120000],\n",
        " [\"6\", \"Moni\", \"sales\", 23000], [\"7\", \"Borno\", \"sales\", 34000],\n",
        " [\"8\", \"Shreya\", \"business\", 456798], [\"9\", \"Kishor\", \"IT\", 230000],\n",
        " [\"10\", \"Ron\", \"business\", 100000] ]\n",
        "# specify column names\n",
        "columns = ['ID', 'NAME', 'sector', 'salary']\n",
        "# creating a dataframe from the lists of data\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "# display dataframe\n",
        "dataframe.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e4wVsliTcGL",
        "outputId": "df09c500-e9f1-426a-8cf4-7d2f07dd6368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------+------+\n",
            "| ID|  NAME|  sector|salary|\n",
            "+---+------+--------+------+\n",
            "|  1|Sourav|      IT| 45000|\n",
            "|  2| Aditi|      IT| 30000|\n",
            "|  3| Bobby|business| 45000|\n",
            "|  4| Rohit|      IT| 45000|\n",
            "|  5|Shamim|business|120000|\n",
            "|  6|  Moni|   sales| 23000|\n",
            "|  7| Borno|   sales| 34000|\n",
            "|  8|Shreya|business|456798|\n",
            "|  9|Kishor|      IT|230000|\n",
            "| 10|   Ron|business|100000|\n",
            "+---+------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.select('NAME').where(dataframe.ID>5).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Fbz0daTfxk",
        "outputId": "026b02fc-5662-482c-9c5c-a861d3b63420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 1\n"
      ],
      "metadata": {
        "id": "pt-upIJ6Tu5D"
      }
    }
  ]
}