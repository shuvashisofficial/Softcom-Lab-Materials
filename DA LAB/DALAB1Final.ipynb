{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-00X_ceshhZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the Spark file to the current folder\n",
        "!tar xf spark-3.0.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olgTQgxgso_2",
        "outputId": "ec44c2b5-31b5-4102-b4ac-89f7a04824d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Spark\n",
        "df = spark.createDataFrame([{\"Its\": \"my life\"} for x in range(1000)])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CJK8NPxssUm",
        "outputId": "ef99c9d1-14e7-4fd7-e0f0-83fbb901606b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|    Its|\n",
            "+-------+\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "|my life|\n",
            "+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('/content/Datacamp_Ecommerce.csv',header=True,escape=\"\\\"\")\n",
        "df.show(5,0)\n",
        "print(df.count())"
      ],
      "metadata": {
        "id": "Lps4Sr_Vs029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9c24fb-2bc3-46b1-ba0e-3ca25f4e8a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
            "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
            "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
            "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n",
            "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
            "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
            "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "474379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [['a', 1, 2], ['b', 2, 3],['c', 3, 4]]\n",
        "col_name = ['A', 'B', 'C']\n",
        "spark.createDataFrame(my_list, col_name).show()\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(my_list,columns= col_name)\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZWV6Zqsunfl",
        "outputId": "616c6889-fa80-4679-8d4a-ecc469233af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "|  A|  B|  C|\n",
            "+---+---+---+\n",
            "|  a|  1|  2|\n",
            "|  b|  2|  3|\n",
            "|  c|  3|  4|\n",
            "+---+---+---+\n",
            "\n",
            "   A  B  C\n",
            "0  a  1  2\n",
            "1  b  2  3\n",
            "2  c  3  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# df = spark.createDataFrame(pandas_df.toPandas())\n",
        "# Creating pandas dataframe first\n",
        "l = [[\"2015-06-23\", 5]\n",
        " ,[\"2016-07-20\", 7]] #List with data elements\n",
        "df = spark.createDataFrame(pd.DataFrame(l),['data_date','months_to_add'])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_lSLAopuwIM",
        "outputId": "eda01e78-f195-42e0-9abe-4c312de5a6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:327: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# df = spark.createDataFrame(pandas_df.toPandas())\n",
        "# Creating pandas dataframe first\n",
        "#List with data elements\n",
        "df = spark.createDataFrame(pd.DataFrame(my_list),(col_name))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wjA9H1uvMkT",
        "outputId": "70581b67-8e32-41ab-93a9-3066adc04e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:327: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "|  A|  B|  C|\n",
            "+---+---+---+\n",
            "|  a|  1|  2|\n",
            "|  b|  2|  3|\n",
            "|  c|  3|  4|\n",
            "+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QA 3\n"
      ],
      "metadata": {
        "id": "y3Jgedl8vGNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spark libraries\n",
        "from pyspark.sql import Row, DataFrame\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
        "# create RDD to load into spark dataframe\n",
        "l = [[\"2015-06-23\", 5],[\"2016-07-20\", 7]] #List with data elements\n",
        "rdd1 = spark.sparkContext.parallelize(l)\n"
      ],
      "metadata": {
        "id": "eBMwjriOwPUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([ StructField(\"data_date\", StringType(), True),\n",
        "StructField(\"months_to_add\", IntegerType(), True)]) # Col, Type, Nullable\n",
        "df = spark.createDataFrame(rdd1, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toRs9yLOxAZT",
        "outputId": "8961702d-7355-4978-8fd9-e2e6ca5407ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spark libraries\n",
        "from pyspark.sql import Row, DataFrame\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
        "# create RDD to load into spark dataframe\n",
        "l = [[\"2015-06-23\", 5] , [\"2016-07-20\", 7]] #List with data elements\n",
        "rdd1 = spark.sparkContext.parallelize(l)\n",
        "row_rdd = rdd1.map(lambda x: Row(x[0], x[1]))\n",
        "df = spark.createDataFrame(row_rdd, ['data_date', 'months_to_add'])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJkdrXaixmCi",
        "outputId": "b54ea869-4170-4d78-c6ef-7fd270317b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}