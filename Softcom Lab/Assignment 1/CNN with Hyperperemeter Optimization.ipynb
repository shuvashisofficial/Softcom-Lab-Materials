{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('No GPU detected')\n",
    "\n",
    "num_gpus = len(physical_devices)\n",
    "\n",
    "if num_gpus > 0:\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {tf.config.experimental.get_device_details(physical_devices[0])}\")\n",
    "else:\n",
    "    print(\"No GPUs available\")\n",
    "\n",
    "device = tf.device('gpu:0' if len(physical_devices) > 0 else 'cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'd:\\\\SoftCom_Assignment01\\\\Dataset'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: 02 & 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = keras.utils.image_dataset_from_directory(data_dir, validation_split = 0.1, subset = 'training', seed = 1, shuffle = True, batch_size = 32, image_size=(256,256))\n",
    "\n",
    "test_data = keras.utils.image_dataset_from_directory(data_dir, validation_split = 0.1, subset = 'validation', seed = 1, shuffle = True, batch_size = 32, image_size=(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = pathlib.Path(data_dir)\n",
    "for label in train_data.class_names :\n",
    "    images = list(filenames.glob(f'{label}/*'))\n",
    "    print(f'{label} : {len(images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.cardinality().numpy(),  test_data.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_data.take(130)\n",
    "val_set = train_data.skip(130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.cardinality().numpy(), val_set.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print random images from the train set\n",
    "plt.figure(figsize = (15, 15))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(15):\n",
    "        index = random.randint(0, len(images))\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        plt.imshow(images[index].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_data.class_names[labels[index]], color= 'blue', fontsize= 12)\n",
    "        plt.axis(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images_batch, labels_batch in train_set:\n",
    "    print(images_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: 04 & 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(128,(3,3), activation='relu', input_shape=(256,256,3)), # Hidden Layer 1\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Conv2D(64,(3,3), activation='relu'), # Hidden Layer 2\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Conv2D(32,(3,3), activation='relu'), # Hidden Layer 3\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Flatten(), # Output layer\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(), optimizer = keras.optimizers.Adam(), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model.fit(train_set, epochs=25, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel_dummy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(128,(3,3), activation='relu', input_shape=(256,256,3)), # Hidden Layer 1\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Conv2D(64,(3,3), activation='relu'), # Hidden Layer 2\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Conv2D(32,(3,3), activation='relu'), # Hidden Layer 3\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Flatten(), # Output layer\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(), optimizer = keras.optimizers.Adam(), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model.fit(train_set, epochs=15, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel_dummy2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16,(3,3), activation='relu', input_shape=(256,256,3)), # Hidden Layer 1\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Conv2D(32,(3,3), activation='relu'), # Hidden Layer 2\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Flatten(), # Output layer\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "def create_model(trial):\n",
    "    model = keras.Sequential([\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.Conv2D(trial.suggest_int('conv1_filters', 32, 128), (3,3), activation='relu', input_shape=(256,256,3)),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Conv2D(trial.suggest_int('conv2_filters', 32, 128), (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Conv2D(trial.suggest_int('conv3_filters', 32, 128), (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(trial.suggest_float('dropout1_rate', 0.0, 0.5)),\n",
    "        layers.Dense(trial.suggest_int('dense1_units', 64, 256), activation='relu'),\n",
    "        layers.Dropout(trial.suggest_float('dropout2_rate', 0.0, 0.5)),\n",
    "        layers.Dense(trial.suggest_int('dense2_units', 64, 256), activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Use your train_set and val_set here\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    history = model.fit(train_set, epochs=15, validation_data=val_set, verbose=0)\n",
    "    \n",
    "    return -history.history['val_accuracy'][-1]  # Minimize negative validation accuracy\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Best hyperparameters from the Optuna trials\n",
    "best_params = {\n",
    "    'conv1_filters': 32,\n",
    "    'conv2_filters': 64,\n",
    "    'conv3_filters': 128,\n",
    "    'dropout1_rate': 0.5,\n",
    "    'dense1_units': 128,\n",
    "    'dropout2_rate': 0.5,\n",
    "    'dense2_units': 64,\n",
    "}\n",
    "\n",
    "# Define the CNN model using the best hyperparameters\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(best_params['conv1_filters'], (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(best_params['conv2_filters'], (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(best_params['conv3_filters'], (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(best_params['dropout1_rate']),\n",
    "    layers.Dense(best_params['dense1_units'], activation='relu'),\n",
    "    layers.Dropout(best_params['dropout2_rate']),\n",
    "    layers.Dense(best_params['dense2_units'], activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_optimise_final = model.fit(train_set, epochs=25, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel_optimised_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimised_final = tf.keras.models.load_model('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel_optimised.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model_optimised_final.evaluate(test_data, verbose= 1)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = None, None\n",
    "for images, labels in test_data.take(500):\n",
    "    if X_test == None or y_test == None:\n",
    "        X_test = images\n",
    "        y_test = labels\n",
    "    else:\n",
    "        X_test = tf.concat([X_test, images], axis = 0)\n",
    "        y_test = tf.concat([y_test, labels], axis = 0)\n",
    "        \n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model_optimised_final.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Best hyperparameters obtained from Optuna\n",
    "best_hyperparameters = {\n",
    "    'conv1_filters': 121,\n",
    "    'conv2_filters': 66,\n",
    "    'conv3_filters': 127,\n",
    "    'dropout1_rate': 0.3564713596268573,\n",
    "    'dense1_units': 152,\n",
    "    'dropout2_rate': 0.15905312684947298,\n",
    "    'dense2_units': 98\n",
    "}\n",
    "\n",
    "# Create the CNN model with the best hyperparameters\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(best_hyperparameters['conv1_filters'], (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(best_hyperparameters['conv2_filters'], (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(best_hyperparameters['conv3_filters'], (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(best_hyperparameters['dropout1_rate']),\n",
    "    layers.Dense(best_hyperparameters['dense1_units'], activation='relu'),\n",
    "    layers.Dropout(best_hyperparameters['dropout2_rate']),\n",
    "    layers.Dense(best_hyperparameters['dense2_units'], activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics='accuracy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_optimise = model.fit(train_set, epochs=25, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel_optimised.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images_batch, labels_batch in train_set:\n",
    "    print(images_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16,(3,3), activation='relu', input_shape=(256,256,3)), # Hidden Layer 1\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Conv2D(32,(3,3), activation='relu'), # Hidden Layer 2\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Flatten(), # Output layer\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(), optimizer = keras.optimizers.Adam(), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set, epochs=15, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimised = tf.keras.models.load_model('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel_optimised.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.load_model('d:\\\\SoftCom_Assignment01\\\\Model\\\\cnnModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimised.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history_df):\n",
    "    plt.figure(figsize = (13, 4), dpi = 120)\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['loss'], marker = '.', label = 'Training Loss')\n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['val_loss'], marker = '^', label = 'Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    ax = plt.subplot(1, 2, 2) \n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['accuracy'], marker = '.', label = 'Training Accuracy')\n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['val_accuracy'], marker = '^', label = 'Validation Accurcay')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(pd.DataFrame(history_optimise.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = None, None\n",
    "for images, labels in test_data.take(200):\n",
    "    if X_test == None or y_test == None:\n",
    "        X_test = images\n",
    "        y_test = labels\n",
    "    else:\n",
    "        X_test = tf.concat([X_test, images], axis = 0)\n",
    "        y_test = tf.concat([y_test, labels], axis = 0)\n",
    "        \n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model_optimised.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model_optimised.evaluate(test_data, verbose= 1)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.6f}\")\n",
    "print(f\"Precision: {precision:.6f}\")\n",
    "print(f\"Recall: {recall:.6f}\")\n",
    "print(f\"F1 Score: {f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Plot confusion matrix\n",
    "labels = ['covid', 'normal', 'pneumonia']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot random images from a given dataset, and compare predictions with ground truth\n",
    "def plot_random_predictions(dataset, model_optimised):\n",
    "\n",
    "    shuffled_data = dataset.shuffle(10)\n",
    "    class_names = dataset.class_names\n",
    "\n",
    "    for images, labels in shuffled_data.take(1):\n",
    "        plt.figure(figsize = (10, 10), dpi = 120)\n",
    "        y_pred_proba = model_optimised.predict(images)\n",
    "\n",
    "    for i in range(9):\n",
    "        index = random.randint(0, len(images))\n",
    "        ax = plt.subplot(3,3, i + 1)\n",
    "\n",
    "        img = images[index].numpy().astype(\"uint8\")\n",
    "        y_true = class_names[labels[index]]\n",
    "        y_pred = class_names[np.argmax(y_pred_proba[index], axis = 0)]\n",
    "      \n",
    "        c = 'g' if y_pred == y_true else 'r'\n",
    "      \n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Predicted : {y_pred}\\nTrue label : {y_true}', c = c)\n",
    "        plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions(test_data, model_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ghost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
